{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tweepy\n",
    "import datetime\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "import nltk\n",
    "from rake_nltk import Rake\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "OPEN_AI_KEY = \"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\"\n",
    "TWITTER_API_KEY = \"jNbc56iZbzSmBjZV4BgKFBRqf\"\n",
    "TWITTER_API_SECRET = \"7HcuH2EwgrwyKkZ20RbVpKtQqyauKa7wHkK1VvITyiIql2Sbfp\"\n",
    "TWITTER_BEARER = r\"AAAAAAAAAAAAAAAAAAAAAOTYkgEAAAAAmYYQdgPQuqO%2FnTsSsgzZlhvggzo%3DfgzyE5I08GTEJKzxZLUf26OT5hQWNcDoz9LqyI2c2B2HyHAlkP\"\n",
    "TWITTER_TOKEN= \"1127693381965099013-yhBB7JUofZuUCyej2fUsFVJgLz5vOs\"\n",
    "TWITTER_TOKEN_SECRET = \"pQh7moipvtKizXWoTFHFHddcfGOzfQsxoLAIGMG2Vn956\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to clean tweets\n",
    "def clean_tweet_text(tweet_text):\n",
    "    # Remove hyperlinks\n",
    "    tweet_text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", tweet_text)\n",
    "    \n",
    "    # Remove emojis and other non-alphanumeric characters except \"@\"\n",
    "    tweet_text = tweet_text.encode(\"ascii\", \"ignore\").decode(\"utf-8\")\n",
    "    tweet_text = re.sub(r\"[^\\w\\s@#]\", \"\", tweet_text)\n",
    "    \n",
    "    return tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to scrape tweets\n",
    "def scrape_user_tweets(username, start_date_str, end_date_str, tweet_limit):\n",
    "    consumer_key = TWITTER_API_KEY\n",
    "    consumer_secret = TWITTER_API_SECRET\n",
    "    access_token = TWITTER_TOKEN\n",
    "    access_token_secret = TWITTER_TOKEN_SECRET\n",
    "\n",
    "    # Authenticate to Twitter\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "    # Create API object\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    # Convert date strings to offset-aware datetime objects\n",
    "    start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "    end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "\n",
    "    # Scrape user's tweets\n",
    "    tweets = []\n",
    "    for tweet in tweepy.Cursor(api.user_timeline, screen_name=username, tweet_mode=\"extended\").items(tweet_limit):\n",
    "        tweet_date = tweet.created_at.replace(tzinfo=timezone.utc)\n",
    "        if tweet_date < start_date or tweet_date > end_date:\n",
    "            if (not tweet.retweeted and \"RT\" not in tweet.full_text and tweet.in_reply_to_status_id is None):\n",
    "                tweet.full_text = clean_tweet_text(tweet.full_text)\n",
    "                tweets.append(tweet)\n",
    "\n",
    "    # Create DataFrame from tweets\n",
    "    data = {\n",
    "        \"Tweet ID\": [tweet.id for tweet in tweets],\n",
    "        \"Created At\": [tweet.created_at for tweet in tweets],\n",
    "        \"Text\": [tweet.full_text for tweet in tweets]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to scrape tweets with snscrape\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "from datetime import datetime, timedelta\n",
    "def scrape_user_tweets(username, start_date, end_date):\n",
    "    tweets = []\n",
    "    query = f'from:{username} since:{start_date} until:{end_date} -filter:retweets'\n",
    "\n",
    "    # Iterate through each tweet using snscrape\n",
    "    for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "        tweet.content = clean_tweet_text(tweet.content)\n",
    "        tweets.append(tweet.content)\n",
    "\n",
    "    # Create a DataFrame from the scraped tweets\n",
    "    df = pd.DataFrame(tweets, columns=['Tweet'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape hashtags with snscrape\n",
    "def scrape_tweets_by_hashtags(hashtags, start_date, end_date):\n",
    "    # Define the search query\n",
    "    query = ' OR '.join(['#' + hashtag for hashtag in hashtags])\n",
    "\n",
    "    # Scrape tweets\n",
    "    tweets = []\n",
    "    for tweet in sntwitter.TwitterSearchScraper(query + ' -filter:retweets since:' + start_date + ' until:' + end_date).get_items():\n",
    "        tweet.content = clean_tweet_text(tweet.content)\n",
    "        tweets.append(tweet.content)\n",
    "\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean language \n",
    "import langdetect\n",
    "from langdetect import detect\n",
    "\n",
    "def remove_non_english_rows(df, text_column):\n",
    "    # Create an empty list to store indices of rows to be removed\n",
    "    rows_to_remove = []\n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        text = row[text_column]        \n",
    "        # Detect the language of the text\n",
    "        try:\n",
    "            language = detect(text)\n",
    "            \n",
    "            # Check if the detected language is not English\n",
    "            if language != 'en':\n",
    "                rows_to_remove.append(index)\n",
    "        except:\n",
    "            # Handle any errors during language detection\n",
    "            pass\n",
    "    \n",
    "    # Remove the rows from the DataFrame\n",
    "    cleaned_df = df.drop(rows_to_remove)\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the params\n",
    "username = \"hen_ease\"\n",
    "start_date_str = \"2022-01-01\"\n",
    "end_date_str = \"2023-01-01\"\n",
    "tweet_limit = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping tweets\n",
    "scraped_tweets = scrape_user_tweets(username, start_date_str, end_date_str, tweet_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping tweets with snscrape\n",
    "import pandas as pd\n",
    "username = 'hen_ease'\n",
    "start_date = '2018-06-01'\n",
    "end_date = '2023-06-26'\n",
    "\n",
    "scraped_tweets = scrape_user_tweets(username, start_date, end_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = ['bostonceltics', 'newenglandpatriots', 'bostonbruins', 'bostonredsox', 'PardonMyTake']\n",
    "start_date = '2022-06-01'\n",
    "end_date = '2023-06-26'\n",
    "\n",
    "scraped_tweets_hashtags = scrape_tweets_by_hashtags(hashtags, start_date, end_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_tweets = pd.DataFrame(scraped_tweets_hashtags, columns=['Tweet'])\n",
    "hashtag_tweets = remove_non_english_rows(hashtag_tweets, 'Tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keywords functions\n",
    "def extract_top_keywords(sentence):\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(sentence)\n",
    "    keywords = r.get_ranked_phrases()\n",
    "    if not keywords:\n",
    "        keywords.append(\"Nothing\")\n",
    "    phrase = \"Write a Tweet in the words of Hank Lockwood about \" + keywords[0]\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tweet summaries\n",
    "scraped_tweets = scraped_tweets[scraped_tweets['Tweet'] != '']\n",
    "scraped_tweets['Summary'] = scraped_tweets['Tweet']\n",
    "scraped_tweets['Summary'] = scraped_tweets['Summary'].map(extract_top_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_tweets1 = pd.DataFrame(hashtag_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tweet summaries\n",
    "hashtag_tweets1 = hashtag_tweets1[hashtag_tweets1['Tweet'] != '']\n",
    "hashtag_tweets1['Summary'] = hashtag_tweets1['Tweet']\n",
    "hashtag_tweets1['Summary'] = hashtag_tweets1['Summary'].map(extract_top_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine hashtags and hank tweets\n",
    "frames = [scraped_tweets, hashtag_tweets1]\n",
    "merged = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json file function\n",
    "def create_json_file(df, prompt_column, completion_column, output_file):\n",
    "    data = []\n",
    "    for _, row in df.iterrows():\n",
    "        prompt_text = row[prompt_column]\n",
    "        completion_text = row[completion_column]\n",
    "        entry = {\"prompt\": prompt_text, \"completion\": completion_text}\n",
    "        data.append(entry)\n",
    "\n",
    "    with open(output_file, 'w') as file:\n",
    "        json.dump(data, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert all to string \n",
    "def convert_integers_to_lowercase_strings(df, column_name):\n",
    "    df[column_name] = df[column_name].astype(str).str.lower()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_json_file(scraped_tweets, 'Summary', 'Tweet', 'test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(scraped_tweets['Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb2a5fcc046b526cb96108908ed14c839aad88c516edbce3395598721455d178"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
